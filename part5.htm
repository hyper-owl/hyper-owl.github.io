<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2. 관련연구</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part4.htm">&lt; 이전</a><span> | </span><a href="../실험.html">목차</a><span> | </span><a href="part6.htm">다음 &gt;</a></p><h1 style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">2. 관련연구</h1><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 128%;text-align: justify;">쿼리 중심의 다중 문서 요약(QF-MDS)에 대한 기존 연구는 주로 추출 방식에 기반 을 두고 있으며, 일반적으로 문서 집합을 입력으로 받아 쿼리와 가장 관련성이 높 은 문장을 선택하여 요약문을 만드는 방식이다. 이러한 접근은 인공신경망이 자연 어 처리 분야에 적극적으로 사용되기 이전부터 존재하던 접근법으로 문장의 주제를 중심으로 manifold-ranking 알고리즘을 도입하여 다수의 문장에서 중요한 문장들 을 추출해내는 등 문장의 중요도(relevance and centrality)를 중심으로 요약문을 만드는 방식이다[2][3].</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 128%;text-align: left;">그러나 최근에는 자연어 처리 분야에서 혁신적인 발전을 가져온 Transformer 기반 모델의 발전과 GPT로 대변되는 대규모 언어모델의 등장으로 굉장히 큰 언어모델을 사전학습 시킨 후 이러한 모델을 fine-tuning하여 사용하는 방식이 주를 이루고 있 다. 구글에서 개발한 문서 요약을 위한 모델인 PEGASUS (Pre-training with Extracted   Gap-sentences   for   Abstractive   SUmmarization Sequence-to-sequence) 또한 이런 방식의 대표적인 모델이고 추출 기반과 생성 기반 요약 모두를 지원한다. DUC 데이터셋을 활용한 다양한 평가에서 최상위 성능 을 보여주고 있으며, 현재 가장 성능이 우수한 모델 중 하나로 평가되고 있다[4]. 이러한 연구들과 달리 이번 연구에서 사용하는 coarse-to-fine 접근법의 경우, 의 미적으로 중요한 text segment를 선택하는 과정과 쿼리를 매칭하는 작업을 분리하 여 진행한다. 이렇게 큰 하나의 작업을 그 하위 3개의 작업(relevance estimator, evidence estimator, centrality estimator)들로 나눠서 진행함으로써 크게 2가지의 장점이 생긴다. 첫 번째 장점은 기존의 방식보다 더 많은 양의 입력 데이터를 처리 할 수 있다는 점이다. 3개의 sub-task를 진행하면서 점진적으로 요약할 문장이 있 는 text segment의 범위를 줄여나가는 방법을 사용하기 때문에 각각의 단계를 거 치면서 점점 처리하는 데이터의 양이 줄어들게 된다. 이러한 특징 덕분에, 기존의 방식과 비교 하였을 때 동일한 크기의 입력문서를 받더라도 더 적은 연산을 수행한 다는 장점이 있다[1].</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 128%;text-align: justify;">두 번째 장점은 evidence estimator에서 쿼리와 text segment간의 관계를 추정할 때,  question answering task의 일종을 생각하고 다룸으로써 기존의 존재하는 QA（question answering）task의 데이터와 모델들을 사용할 수 있다는 점이다. 이는 QF-MDS의 데이터 부족 문제를 해결하고 쿼리와 text segment간의 관계를 안정적으로 추론하는데 도움을 준다. 또한 기존의 QF-MDS 시스템들이 사용하던 TF-IDF같은 방법들은 상대적으로 짧은 키워드들로 이루어진 쿼리에는 적합하지만 길고 복잡한 쿼리일수록 더 부적합하다. “Coarse-to-Fine Query Focused Multi-Document Summarization”의 저자들은 더 길고 복잡한 쿼리를 다루는데 있 어서 기존의 방식보다 본인들의 제안한 coarse-to-fine 접근법이 더 효율적임을 논문에서 주장한다[1].</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 128%;text-align: justify;">그림 1: 위 그림은 기존에 존재하던 다중 문서 요약 시스템의 흐름과 coarse-to-fine 접근법의 흐름을 순서도로 표현한 그림이다. 위에 그려진 기 존 방식의 경우, relevance estimator가 centrality estimator에 종속되어서 한번에 진행되는 것을 알 수 있다. 그러나 아래에 그려진 coarse-to-fine 접 근법의 경우, 각각이 모듈별로 순차적으로 진행되는 것을 볼 수 있다. coarse-to-fine 접근법에서 각각의 모듈을 통과하고 난 뒤, 요약문에 포함되 지 않는다고 판단한 text segment는 다음 단계로 보내지 않기 때문에 모듈을 통과할 때마다 처리하는 데이터의 양은 줄어들게 된다.</p><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 128%;text-align: justify;">한국어 문서 요약 연구의 경우, 성능을 인정받은 영어 모델의 구조를 가져와서 한 국어로 다시 학습시킨 모델들이 대부분이다. 이러한 예시로 BART의 구조를 가져와 서 40GB 이상의 한국어 텍스트에 대해서 학습시켜 만든 SKT의 KoBART모델과 “Text Summarization with Pretrained Encoders”논문에서 소개된 BertSum모델 을 한국어 데이터에 적용할 수 있도록 변형시킨 KoBertSum모델 등이 있다. 이번 졸업작품에서는 이러한 한국어 문서 요약 모델과 성능평가를 진행하여 만든 모델의 성능을 평가할 예정이다. 실제로 많은 한국어 논문에서 앞서 소개한 2개의 모델을 활용하여 다양한 도메인에서 문서 요약 성능을 평가하고 있다[6]. 여기서 더 나아 가 이러한 문서 요약 모델을 중간단계로 활용하거나 변형하여 혐오 발언 분류나 가 짜뉴스 탐지기 등 더 큰 시스템으로 활용하는 연구들도 많이 진행되고 있다[7][8]. 이러한 관점에서 KoBART모델이나 KoBertSum모델 등과 성능평가를 진행하여 비 슷하거나 더 나은 성과를 얻는다면 여러 한국어 문서 요약 연구들과 그를 활용한 다양한 연구들에 기여할 수 있을 것으로 기대한다.</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part4.htm">&lt; 이전</a><span> | </span><a href="../실험.html">목차</a><span> | </span><a href="part6.htm">다음 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
